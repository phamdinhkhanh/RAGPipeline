{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. RAG pipeline introduction"
      ],
      "metadata": {
        "id": "pBiPnsiChnGa"
      },
      "id": "pBiPnsiChnGa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. What is RAG pipeline"
      ],
      "metadata": {
        "id": "-4LAg_AuU4qI"
      },
      "id": "-4LAg_AuU4qI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assume that your company has a plan to build an efficient chatbot that can question and answering not only the general questions but also the specific questions included in your company's documents. It is hard to a LLM can answer the specific questions that it did not study before.\n",
        "\n",
        "The RAG (Retrieval-Augmented Generation) pipeline is an approach in natural language processing (NLP) that helps to retrieve useful information and provide a more precise answer compared with normal LLM generation. It combines information retrieval with language generation techniques. It is also a solution to improve the performance of generative models by incorporating a retriever component."
      ],
      "metadata": {
        "id": "zSns0FGFLbw-"
      },
      "id": "zSns0FGFLbw-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Why is RAG pipeline"
      ],
      "metadata": {
        "id": "N58QqMmMPisc"
      },
      "id": "N58QqMmMPisc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, RAG pipeline demonstrates its prowess of retrieving the relevant contexts from diverse data sources. It is a powerful tool that pushes the capability of a normal LLM to a new frontier. In a nushell, there are three principal advantages of using RAG such as:\n",
        "\n",
        "- **Empowering LLM with real-time data access**:\n",
        "Because the business context always constantly changes over time. Therefore, data is constantly dynamic and transformed in an enterprise that demands AI solutions, which can use LLMs to have the ability to remain up-to-date and current with RAG to facilitate direct access to additional data resources. Ideally, these resources should comprise of real-time and personalized data.\n",
        "\n",
        "- **Preserving data privacy**:\n",
        "Many enterprise data is sensitive and confidential. That is why the commercial LLM models like GPT-4, GPT-3.5, Claude, and BARD are banned in several corporations, especially in the case where data is considered as the new gold. Therefore, ensuring data privacy is crucial for enterprises.To this end, with a self-hosted LLM (demonstrated in the RAG workflow), sensitive data can be retained on-premises just like the local stored data.\n",
        "\n",
        "- **Mitigating LLM hallucinations**:\n",
        "In fact since many LLMs lack access to factual and real-time information, they often generate inaccurate responses but seem convincing. This phenomenon, so-called hallucination, is mitigated by RAG, which reduces the likelihood of hallucinations by providing the LLM with relevant and factional information."
      ],
      "metadata": {
        "id": "cuTsKgG1PuYh"
      },
      "id": "cuTsKgG1PuYh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Where RAG pipeline"
      ],
      "metadata": {
        "id": "WnTaycDfZiuw"
      },
      "id": "WnTaycDfZiuw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is estatic when Large language models (LLMs) have astounded the world with their unprecedented competencies to understand and generate human-like responses. Their chat feature offers a swift and natural interaction between humans and immense amount of data. For instance, they demonstrate an extraordinary capability of summarizing and extracting the highlights from data or replacing functional queries such as SQL queries with natural language commands.\n",
        "\n",
        "It is essential to emphasize that business value can be generated by these models without additional effort, but this is unsually not often the matter. Luckily, all that the users try to distil value out of using LLMs is to foster the LLM with their own data. This can be accomplished with retrieval augmented generation (RAG), which is showcased thorough out this tutorial.\n",
        "\n",
        "By reinforcing an LLM with their business data, enterprises can make their AI applications agile and responsive to the new developments. For instance:\n",
        "\n",
        "- **Chatbots**: Many companies have already used AI chatbots in their customer service to enable the customer to lively interact on their websites day and night. By using RAG pipeline, companies can leverage a tailored chat version that is highly determined to their product and policy. In specific, questions about product specifications could conveniently be handled.\n",
        "\n",
        "- **Customer service**: Companies can authorize live service agents to easily answer customer questions with precise, up-to-date information.\n",
        "\n",
        "- **Enterprise search**: Each enterprises has a wealth of knowledge across the departments that includes company terms, sale policies, IT support articles, and code repositories. That is why employees could seek an internal search engine to get information faster and more precise.\n",
        "\n",
        "In conclusion, this post explains the benefits of using the RAG technique when implementing an LLM application, along with the components of a RAG pipeline in the next section.\n",
        "\n"
      ],
      "metadata": {
        "id": "exuje5NrZzcj"
      },
      "id": "exuje5NrZzcj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4. How does RAG pipeline work?\n",
        "\n",
        "![](https://imgur.com/K9eh8oL.png)"
      ],
      "metadata": {
        "id": "tO45o-oOU_pl"
      },
      "id": "tO45o-oOU_pl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RAG pipeline typically consists of two main components:\n",
        "\n",
        "**1.4.1. Retriever:** The retriever is responsible for selecting relevant passages or document chunks from a large corpus of text. It uses information retrieval techniques to identify the most informative and contextually relevant pieces of information. This step helps in reducing the search space and focusing on the most suitable content. In general, it includes these main steps:\n",
        "\n",
        "- **Document ingestion**:\n",
        "First, raw data from diverse sources, such as a data source, pdf, text, files, images, or streaming live feeds, are collected into data lake, then ingested to RAG system. The most challenging aspect at this step is the diversity of data types, which requires different pre-processing technologies for each one. To this end, LangChain offers a variety of document loaders that load data for many forms from diverse sources. The term document loader is used loosely. Source documents do not necessarily need to be what you might think of as standard documents (PDFs, text files, and so on). That is why LangChain supports loading data from Confluence, CSV files, Outlook emails, and [more](https://python.langchain.com/docs/integrations/document_loaders). LlamaIndex also provides a variety of loaders, which can be viewed in [LlamaHub](https://llamahub.ai/).\n",
        "\n",
        "- **Document pre-processing**:\n",
        "documents are often transformed after they have been loaded in the step Document Ingestion. One ordinary transformation method is text-splitting, which split down long-form document into many continous smaller segments. This is esential for embedding the text by embedding model, for example `e5-large-v2` or `BAAI/bge-large-en`, which has a maximum token length of 512 and 1024, respectively. One noteworthy caution to consider is that splitting may lead to missing out on information. Therefore, expanding your retrieval segment or splitting text under the overlapping style may be very useful in elevating the relevance of the extracted context.\n",
        "\n",
        "- **Generating embeddings**:\n",
        "Data must be transformed into a due format that the system can efficiently process. Generating embeddings involves converting data into high-dimensional vectors, which represent text in a numerical format.\n",
        "\n",
        "- **Storing embeddings in vector databases**:\n",
        "The processed data and generated embeddings are stored and indexed in distinctive databases known as vector indexing databases. These databases are optimized to save and seek vectorized data, enabling fast search and retrieval operations. Storing the data in accelerated vector databases like Chroma, Pipecone, and Milvus ensures that information remains accessible and can be rapidly retrieved during real-time querying.\n",
        "\n",
        "**1.4.2. Generator:** The generator is a language model that takes the retrieved passages as input and generates coherent and contextually appropriate responses. It can be based on transformer architectures like GPT (Generative Pre-trained Transformer) or similar models that excel in natural language understanding and generation tasks. There are two main steps included in this phase:\n",
        "\n",
        "- **LLMs**:\n",
        "LLMs account for a foundational generative component of the RAG pipeline. These large language models are trained on vast datasets, enabling them to comprehend and anwser human-like text. In terms of RAG pipeline, LLMs are used to germinate fully formed responses based on the user query and contextualized information extracted from the vector DBs during real-time interactions.\n",
        "\n",
        "- **Querying**:\n",
        "When a user send a query, the RAG system uses the pre-indexed chunk vectors and input embedding vector to perform efficient searches based on similarity scoring. The system identifies relevant information by comparing the query vector with the stored vectors in the vector databases. The LLMs then use the retrieved data to shape human-like responses.\n",
        "\n",
        "\n",
        "In conclusion, by combining retrieval and generation, the RAG pipeline aims to leverage the benefits of both approaches. Retrieval helps in extracting relevant information from a large dataset, while generation allows for the creation of diverse and contextually appropriate responses."
      ],
      "metadata": {
        "id": "Si9GMeKsVDk8"
      },
      "id": "Si9GMeKsVDk8"
    },
    {
      "cell_type": "markdown",
      "id": "44349a83-e1dc-4eed-ba75-587f309d8c88",
      "metadata": {
        "id": "44349a83-e1dc-4eed-ba75-587f309d8c88"
      },
      "source": [
        "# 2. Build RAG pipeline using OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Build RAG pipeline on unstructure data"
      ],
      "metadata": {
        "id": "RI4-_JqxKTPD"
      },
      "id": "RI4-_JqxKTPD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1. Setup\n",
        "\n",
        "We’ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever."
      ],
      "metadata": {
        "id": "6qYzZMfwVJ3I"
      },
      "id": "6qYzZMfwVJ3I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we need to download a list of packages like `langchain, chromadb, openai, tiktoken` for building a RAG pipeline."
      ],
      "metadata": {
        "id": "RldL3PCfVnP7"
      },
      "id": "RldL3PCfVnP7"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.352 unstructured[all-docs] pydantic==1.10.13 lxml==4.9.3\n",
        "!pip install openai==1.6.1 chromadb==0.4.21 tiktoken==0.5.2 langchainhub==0.1.14"
      ],
      "metadata": {
        "id": "oXH5ifenVoj3"
      },
      "id": "oXH5ifenVoj3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZuZhOQQpTpEX"
      },
      "id": "ZuZhOQQpTpEX"
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI Key\"\n",
        "\n",
        "# import dotenv\n",
        "# dotenv.load_dotenv()"
      ],
      "metadata": {
        "id": "44mCaR0NTrM8"
      },
      "id": "44mCaR0NTrM8",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.2. Build RAG pipeline"
      ],
      "metadata": {
        "id": "OB8_SKTuYTkI"
      },
      "id": "OB8_SKTuYTkI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, LLM can not answer very specific questions that it have never learned yet. For example, they are closed enterprise's documents, new papers, books,.... Thus, it is helpful to build a an application by using Langchain technology that can answer any question you query on them. To illustrate the first simple case, in this guide we’ll build a QA app over the [Dive into Deep Learning - chapter 3.1](https://d2l.ai/chapter_linear-regression/linear-regression.html), which allows us to ask questions about the contents of the post. We can create a simple indexing pipeline and RAG chain to do this in ~20 lines of code:\n"
      ],
      "metadata": {
        "id": "0QAJtB_DUq-Z"
      },
      "id": "0QAJtB_DUq-Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "bpDAMgd3VWm8"
      },
      "id": "bpDAMgd3VWm8",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://d2l.ai/chapter_linear-regression/linear-regression.html\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"page-content\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "VG8Xn0H4VZel"
      },
      "id": "VG8Xn0H4VZel",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"What is the loss function of Linear Regression?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8xoX5BtjXjkN",
        "outputId": "fc89fa24-bd5e-4046-cf0e-852533ad7db2"
      },
      "id": "8xoX5BtjXjkN",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The loss function of Linear Regression is the squared error. It quantifies the distance between the real and predicted values of the target, and it is given by the formula: \\\\(\\\\frac{1}{2}\\\\left(\\\\mathbf{w}^\\\\top \\\\mathbf{x}^{(i)} + b - y^{(i)}\\\\right)^2\\\\). The goal of training the model is to find the parameters (\\\\(\\\\mathbf{w}^*, b^*\\\\)) that minimize the total loss across all training examples.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.3. Code explaination"
      ],
      "metadata": {
        "id": "U7B06E6sZ0au"
      },
      "id": "U7B06E6sZ0au"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DataLoader"
      ],
      "metadata": {
        "id": "J76HVLYMaoBt"
      },
      "id": "J76HVLYMaoBt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to first load the blog post contents. We can use [DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) for this, which are objects that load in data from a source and return a list of [Documents](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html). A Document is an object with some page_content (str) and metadata (dict).\n",
        "\n",
        "In this case we’ll use the [WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base), which uses urllib to load HTML form web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters to the BeautifulSoup parser via bs_kwargs (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class “page-content” is chosen, so we’ll remove all others."
      ],
      "metadata": {
        "id": "ctQHsnVPZ8id"
      },
      "id": "ctQHsnVPZ8id"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://d2l.ai/chapter_linear-regression/linear-regression.html\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"page-content\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "EnRf1swjafZE"
      },
      "id": "EnRf1swjafZE",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5Ql-P-DbiP3",
        "outputId": "8a869de5-409f-4672-c459-2b3b8292106e"
      },
      "id": "S5Ql-P-DbiP3",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32916"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indexing split"
      ],
      "metadata": {
        "id": "KHwErEguass3"
      },
      "id": "KHwErEguass3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our loaded document is over 33k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
        "\n",
        "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
        "\n",
        "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
        "\n",
        "We set add_start_index=True so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”.\n",
        "\n"
      ],
      "metadata": {
        "id": "brsNkeC1baHJ"
      },
      "id": "brsNkeC1baHJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "IQonAFu1bRux"
      },
      "id": "IQonAFu1bRux",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdKtpj0rcI3h",
        "outputId": "dbd4527b-7bb7-4336-93b1-cd1a567efcec"
      },
      "id": "cdKtpj0rcI3h",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEXCrMgTcKOE",
        "outputId": "912b3ed2-8db1-49ac-f92a-cba9c9fca487"
      },
      "id": "TEXCrMgTcKOE",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "995"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[10].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h9-nRNKcN9z",
        "outputId": "befdb538-0100-4341-e7fa-4b0e904e09a8"
      },
      "id": "0h9-nRNKcN9z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://d2l.ai/chapter_linear-regression/linear-regression.html',\n",
              " 'start_index': 6331}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indexing store"
      ],
      "metadata": {
        "id": "f9JvgOWdbRAd"
      },
      "id": "f9JvgOWdbRAd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to index our 44 text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
        "\n",
        "We can embed and store all of our document splits in a single command using the Chroma vector store and OpenAIEmbeddings model."
      ],
      "metadata": {
        "id": "CbvIG4NicYAE"
      },
      "id": "CbvIG4NicYAE"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "0bHzSViLcryx"
      },
      "id": "0bHzSViLcryx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Retrieval and Generation: Retrieve"
      ],
      "metadata": {
        "id": "2r2nI1WLcwI2"
      },
      "id": "2r2nI1WLcwI2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
        "\n",
        "First we need to define our logic for searching over documents. LangChain defines a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/) interface which wraps an index that can return relevant Documents given a string query.\n",
        "\n",
        "The most common type of Retriever is the [VectorStoreRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore), which uses the similarity search capabilities of a vector store to facillitate retrieval. Any VectorStore can easily be turned into a Retriever with VectorStore.as_retriever():"
      ],
      "metadata": {
        "id": "lRMZnVTXc2yp"
      },
      "id": "lRMZnVTXc2yp"
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
      ],
      "metadata": {
        "id": "zhbfggXSdMJD"
      },
      "id": "zhbfggXSdMJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.invoke(\"What are the loss function of Linear Regression?\")"
      ],
      "metadata": {
        "id": "NPb_OUfAdP-N"
      },
      "id": "NPb_OUfAdP-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(retrieved_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj55W1WGdaww",
        "outputId": "8d737c6e-587b-4015-ff3f-ad8a4d9e9002"
      },
      "id": "nj55W1WGdaww",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRqpoFWSdfC8",
        "outputId": "4b5b39e3-b9b0-43b8-f62d-7a09e8fd89bc"
      },
      "id": "mRqpoFWSdfC8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1.1.2. Loss Function¶\n",
            "Naturally, fitting our model to the data requires that we agree on some\n",
            "measure of fitness (or, equivalently, of unfitness). Loss\n",
            "functions quantify the distance between the real and predicted\n",
            "values of the target. The loss will usually be a nonnegative number\n",
            "where smaller values are better and perfect predictions incur a loss of\n",
            "0. For regression problems, the most common loss function is the squared\n",
            "error. When our prediction for an example \\(i\\) is\n",
            "\\(\\hat{y}^{(i)}\\) and the corresponding true label is\n",
            "\\(y^{(i)}\\), the squared error is given by:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieval and Generation: Generate"
      ],
      "metadata": {
        "id": "ESORA7vEdiwv"
      },
      "id": "ESORA7vEdiwv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n",
        "\n",
        "We’ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM or ChatModel could be substituted in."
      ],
      "metadata": {
        "id": "8FB7vlD3dxE5"
      },
      "id": "8FB7vlD3dxE5"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "UUHq-u-XduIm"
      },
      "id": "UUHq-u-XduIm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use a prompt for RAG that is checked into the [LangChain prompt hub](https://smith.langchain.com/hub/rlm/rag-prompt)."
      ],
      "metadata": {
        "id": "LrTvZreOd3qx"
      },
      "id": "LrTvZreOd3qx"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "ZPVf_2wZd97R"
      },
      "id": "ZPVf_2wZd97R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "example_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxIgO0B5eKWR",
        "outputId": "9d940362-e419-4ae9-ba11-924ab435fa18"
      },
      "id": "fxIgO0B5eKWR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_messages[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8BS4CoBeK7a",
        "outputId": "0f01f15b-d99a-4e8f-e5ee-501bb7e47b5a"
      },
      "id": "Q8BS4CoBeK7a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: filler question \n",
            "Context: filler context \n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use the [LCEL Runnable protocol](https://python.langchain.com/docs/expression_language/) to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box"
      ],
      "metadata": {
        "id": "VrmslvT5eT9q"
      },
      "id": "VrmslvT5eT9q"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "1yD1Yeuyea4p"
      },
      "id": "1yD1Yeuyea4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain.stream(\"What is the loss function of Linear Regression?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "7jiraYylefvy"
      },
      "id": "7jiraYylefvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Build RAG for Vietnamese language using VinaLlama"
      ],
      "metadata": {
        "id": "fIvrmDzeJI_j"
      },
      "id": "fIvrmDzeJI_j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The procedure is the same as pipeline using OpenAI. However, to run [VinaLlama-7b-chat](https://huggingface.co/vilm/vinallama-7b-chat), using T4 free Google Colab may not enough memory for running. We maybe consider to upgrade Colab Pro that enable us to use A100 or V100 GPU for inference.\n",
        "\n",
        "However, `transformers` library power user to inference Large Language Model up to 7B parameters on T4 GPU (with 16 VRAM) by using quantization techniques. They help to reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn’t be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization algorithms and it supports 8-bit and 4-bit quantization with `bitsandbytes`.\n",
        "\n"
      ],
      "metadata": {
        "id": "IgbVqRt-7X3G"
      },
      "id": "IgbVqRt-7X3G"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.34.0\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install bitsandbytes==0.41.3\n",
        "!pip install llama-cpp-python==0.2.26\n",
        "!pip install accelerate==0.25.0"
      ],
      "metadata": {
        "id": "y8ucTwP_MS-8"
      },
      "id": "y8ucTwP_MS-8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
        "from transformers import pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "pFVJK57XJPX0"
      },
      "id": "pFVJK57XJPX0",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config model to load under 4-bit quantization\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Loading model for generating text\n",
        "model_name = \"vilm/vinallama-2.7b-chat\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=nf4_config\n",
        ")\n",
        "\n",
        "# Create text generation pipeline from causual large language model\n",
        "# Config model to have 512 new tokens.\n",
        "question_answerer = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# Create a huggingface pipeline for question and answering task\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=question_answerer,\n",
        "    model_kwargs={\"temperature\": 0.7},\n",
        ")"
      ],
      "metadata": {
        "id": "rixksT05u-Yz"
      },
      "id": "rixksT05u-Yz",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having a standard pipeline for question and answering, we can test model on a specific question by wrapping them in a prompt template which is suitable for question and answering task.\n",
        "\n",
        "Each model will have its own standard way to config prompt of chat style. With `VinaLlama-7b-chat` model, the prompt includes three agents system, user, and assistant, which are under this format:\n",
        "\n",
        "```\n",
        "<|im_start|>system\n",
        "Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "```\n",
        "\n",
        "Each message of one agent is wrapped up inside `<|im_start|><im_end>` tag.\n",
        "\n",
        "`system` is a message from the system that is fixed throughout the chat application. It always is prepended on top of any message sent to the chat application.\n",
        "\n",
        "`user` is message from humman, it is usually a question or task what you want to ask for.\n",
        "\n",
        "`assistant` is an activating word for answering."
      ],
      "metadata": {
        "id": "bt-ASJMrYRqR"
      },
      "id": "bt-ASJMrYRqR"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Ai là thủ tướng của Việt Nam?\"\n",
        "\n",
        "prompt=f\"\"\"\n",
        "<|im_start|>system\n",
        "Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{query}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)\n",
        "\n",
        "response = llm.predict(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IVro28B0z5V",
        "outputId": "3b5be067-589a-4d85-9cf0-b2175bc1f7f0"
      },
      "id": "7IVro28B0z5V",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|im_start|>system\n",
            "Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "Ai là thủ tướng của Việt Nam?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Thủ tướng hiện tại của Việt Nam là Phạm Minh Chính. Ông đã được bổ nhiệm làm Thủ tướng Chính phủ vào ngày 1 tháng 1 năm 2021, sau khi kế nhiệm của người tiền nhiệm là Nguyễn Xuân Phúc. \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result demonstrates that model can give a proper answer for the input question. Consequently, a rag chain pipeline can be designed to extract relevant context, format them to proper format, then feed the whole template to the large language model. Let's go through step-by-step to shed the light on the way of building this rag pipeline."
      ],
      "metadata": {
        "id": "v5c08sIidzKU"
      },
      "id": "v5c08sIidzKU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Loading data from website and split text into multiple chunks with chunk size is 1000 and overlapping size is 200."
      ],
      "metadata": {
        "id": "_Znqh3cpVaal"
      },
      "id": "_Znqh3cpVaal"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://phamdinhkhanh.github.io/deepai-book/intro.html\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"tex2jax_ignore mathjax_ignore section\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "_ENoQoQKe8md"
      },
      "id": "_ENoQoQKe8md",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Building a question and answering pipeline based on embedding model and LLM, which are in charge of finding relevant retrieval contexts and generating the answer according to in-context learning, respectively. For Vietnamese Language, we can use [VoVanPhuc/sup-SimCSE-VietNamese-phobert-base](https://huggingface.co/VoVanPhuc/sup-SimCSE-VietNamese-phobert-base) as an open-source huggingface embedding model, this model is trained by supervised learning technique. To generate answer for general purpose, we can load pretrained model [vilm/vinallma-7b-chat](https://huggingface.co/vilm/vinallama-2.7b) on huggingface. This model is trained under chat style, thus, it is particularly suitable for question and answering task."
      ],
      "metadata": {
        "id": "7EEoxLVaV-Co"
      },
      "id": "7EEoxLVaV-Co"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Define model name of question and answering\n",
        "model_name = \"vilm/vinallama-7b-chat\" # or \"vilm/vinallama-2.7b-chat\" for a lower version\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model under 4bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=nf4_config\n",
        ")\n",
        "\n",
        "# Create a tokenizer object by loading the pretrained \"vilm/vinallama-7b-chat\" tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
        "# with additional model-specific arguments (temperature and max_length)\n",
        "question_answerer = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=question_answerer,\n",
        "    model_kwargs={\"temperature\": 0.7},\n",
        ")"
      ],
      "metadata": {
        "id": "4gOs8ObAV-ml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c11904d3512f4e408c449ab57fbc12b1",
            "38d08ca820874029aac2e1c22cbf2260",
            "34543e5b25c746f3a626961590e1be02",
            "9aec74d8300a4315a69609202b1f39e2",
            "9253607e5c364e7bbcb40cdab0f5bbfc",
            "8066c5a944e34f6688d392536324eda5",
            "3b78fe2b6a574320837094eb7c23d322",
            "8ec3eedf1d0a4a2abd5d88b01f87c40d",
            "90644ebec4dc4c6ca9faff73576cf5b3",
            "a5217a31a6714597bddfa003b0258445",
            "69afad9ef50147f2946d49ae916715b5"
          ]
        },
        "outputId": "0f1031eb-62f4-40ab-d09c-69a2deb8ff1e"
      },
      "id": "4gOs8ObAV-ml",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c11904d3512f4e408c449ab57fbc12b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Building vector indexing database storing the embedding vector and relevant text chunks."
      ],
      "metadata": {
        "id": "WH0OB4bpXs1_"
      },
      "id": "WH0OB4bpXs1_"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "FMyIAZzGXtYH"
      },
      "id": "FMyIAZzGXtYH",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Building up a rag_chain by LCEL runnable protocal."
      ],
      "metadata": {
        "id": "R3qOuRyFY2dn"
      },
      "id": "R3qOuRyFY2dn"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"<|im_start|>system\\n Bạn là một trợ lý ảo cho tác vụ hỏi đáp. Sử dụng những mẩu văn bản được trích xuất để trả lời câu hỏi. Nếu bạn không biết, hãy trả lời tôi không biết.<|im_end|>\"),\n",
        "    (\"human\", \"<|im_start|>user\\n Sử dụng ba câu tối đa và giữ câu trả lời nhất quán.\\nCâu hỏi: {question} \\nBối cảnh: {context} \\nCâu trả lời:<|im_end|>\"),\n",
        "    (\"assistant\", \"<|im_start|>assistant\")\n",
        "])\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"human\", \"Bạn là một trợ lý ảo cho tác vụ hỏi đáp. Sử dụng những mẩu văn bản được trích xuất để trả lời câu hỏi. Nếu bạn không biết, hãy trả lời tôi không biết. Sử dụng ba câu tối đa và giữ câu trả lời nhất quán.\\nCâu hỏi: {question} \\nBối cảnh: {context} \\nCâu trả lời:\")\n",
        "# ])\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "8NvpU8O0Y1t6"
      },
      "id": "8NvpU8O0Y1t6",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "rag_chain.invoke(\"Nội dung chính của quyển sách này là gì?\")"
      ],
      "metadata": {
        "id": "XmK4Y-Ytl-E1"
      },
      "id": "XmK4Y-Ytl-E1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Reference\n",
        "\n",
        "\n",
        "1. [Semi Structured RAG - Langchain tutorial](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb)\n",
        "\n",
        "2. [Multi model RAG - Langchain tutorial](https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb)\n",
        "\n",
        "3. [RAG pipeline huggingface](https://huggingface.co/docs/transformers/model_doc/rag)\n",
        "\n",
        "4. [Demystifying retrieval augmented generation pipelines - Nvidia](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/)\n",
        "\n",
        "5. [Implementing RAG with langchain and huggingface](https://medium.com/international-school-of-ai-data-science/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7)\n",
        "\n",
        "6. [Implement huggingface models using langchain - analyticsvidhya](https://www.analyticsvidhya.com/blog/2023/12/implement-huggingface-models-using-langchain/)"
      ],
      "metadata": {
        "id": "lvlkoBqQgNbj"
      },
      "id": "lvlkoBqQgNbj"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c11904d3512f4e408c449ab57fbc12b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38d08ca820874029aac2e1c22cbf2260",
              "IPY_MODEL_34543e5b25c746f3a626961590e1be02",
              "IPY_MODEL_9aec74d8300a4315a69609202b1f39e2"
            ],
            "layout": "IPY_MODEL_9253607e5c364e7bbcb40cdab0f5bbfc"
          }
        },
        "38d08ca820874029aac2e1c22cbf2260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8066c5a944e34f6688d392536324eda5",
            "placeholder": "​",
            "style": "IPY_MODEL_3b78fe2b6a574320837094eb7c23d322",
            "value": "Downloading generation_config.json: 100%"
          }
        },
        "34543e5b25c746f3a626961590e1be02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ec3eedf1d0a4a2abd5d88b01f87c40d",
            "max": 132,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90644ebec4dc4c6ca9faff73576cf5b3",
            "value": 132
          }
        },
        "9aec74d8300a4315a69609202b1f39e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5217a31a6714597bddfa003b0258445",
            "placeholder": "​",
            "style": "IPY_MODEL_69afad9ef50147f2946d49ae916715b5",
            "value": " 132/132 [00:00&lt;00:00, 6.32kB/s]"
          }
        },
        "9253607e5c364e7bbcb40cdab0f5bbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8066c5a944e34f6688d392536324eda5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b78fe2b6a574320837094eb7c23d322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ec3eedf1d0a4a2abd5d88b01f87c40d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90644ebec4dc4c6ca9faff73576cf5b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5217a31a6714597bddfa003b0258445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69afad9ef50147f2946d49ae916715b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}